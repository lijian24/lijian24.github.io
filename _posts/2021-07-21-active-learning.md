---
layout: post
title: 主动学习初步调研
categories: [Active Learning]
tags: [NLP, Data Augmentation, AL]
description: 主动学习方案持续调研中
---

### 写在前面
&emsp;&emsp;计算机程序等于算法 + 数据结构。近年随着计算资源的不断增长，计算量不断加大，算法能起到的作用不断升高，在一个个计算机细分领域中，算法不断超越人工的效果。从CV到NLP，高效算法不断挑战着之前的SOTA，极力向着更优的效果努力着。算法发展到现在，已经更像是一种军备竞赛了，虽然不知道有用没用，但是如果不发展的话，不知道哪一天就被从而飞来的流弹击中，从此无疾而终。

&emsp;&emsp;接下来，在本文中不讨论正处于军备竞赛中的算法，毕竟一种新的算法框架，一个新的模型架构都有可能会对整个体系造成巨大的影响。有感于前一段时间吴恩达讨论的MLOps，数据好像才是一个不那么走在最前沿的方向所应该讨论的问题，毕竟garbage in, garbage out的理念应该是早已深入人心的。

&emsp;&emsp;在真正的讨论之前，我们先简单看一下算法模型对大家的影响。理论上，算法模型应该是解决一类问题的一个框架，一个好的算法模型必定是从真实场景中来，并最终可以到真实场景中解决问题的方案。但是其实对于大多数不是太前瞻的团队来说，太阳底下无新事，我们解决的问题并没有那么特殊，其实我们需要的就是把最新的模型可以快速在我们自己业务场景下验证，测试，上线部署的能力。所以，一个良好的应用系统，更应该关注融合底层的不同计算资源、计算框架，快速搭建已有的模型，并最终提供对外服务的能力。

&emsp;&emsp;MLOps是一个比较大的问题，至少到目前了解到的为止，这个细分方向上，并没有出现一家独大的情况，大家都还是在自己的应用场景下，搭建自己的计算框架。真的怀念当前Hadoop刚出来的那时候啊，从标准到实现，都有一个规范化的方案，是所有计算机人的一种幸福。哦，不是，毕竟我们还是需要KPI的，真的同意了别人的标准的话，太容易出现任务完成了，但是没有所谓收益的情况。希望这个世界能够越来越好。

### 真的开始
&emsp;&emsp;MLOps的概念比较大，从Google最新的设计来看，整个MLOps过程可以分为下面几个步骤：
1. 算法调研 & 实验开发：主要是RD基于最新的数据分析 & 算法调研，完成在当前问题集合上的代码开发，更多的是在一个中批量数据集上进行的验证，即可以证明模型的有效性，并且相对于全量数据来说，实验成本也比较小。（当然，如果没有一个合适的数据集的话，全量数据也是一种适配方案。）
2. 持续集成：RD开发的代码到代码包的过程，在DL的场景下，特殊情况下可能包含深度模型。
3. 持续部署：
    + 持续训练：基于RD开发的代码包，以及当前的数据集合，特征集合，完成模型的训练，产出最新的模型内容。
    + 模型部署：将持续训练产出的深度模型，以合理化的方案，将线上模型无感地替换为最新的模型。
    + 线上环境监控：持续检测当前模型在最新数据集上的效果，当发现效果有问题时，再次触发整个流程。

&emsp;&emsp;从上面的流程可以看到，其实整个流程中，除了持续训练步骤之外，和之前的DevOps是趋同的，只是其中的底层环境可能更多的是ML或DL相关的环境以及资源。而其中的持续训练，理论上主要受到当前数据的影响，模型框架以及特征集合在模型开发的过程中就已经确定。

### 基本思想
&emsp;&emsp;从上面的初步调研可以发现，在当前而MLOps概念中，最重要的一部分其实是数据管理，或者可以说是样本的管理。如何从一个线上系统中持续、高质量产出模型可用的数据，是其中的一个重要问题。尤其是其中的高质量。

&emsp;&emsp;数据的高质量从概念上来看的话，可以分为两类。
1. 其一是数据质量的高，也就是效果更好的数据。一个典型的认知错误就是：这份数据是人工标注的，所以其准确率是100%。但是面对不一样的问题，不一样的人在不同的场景下，标注效果可能天差地别。尤其是在需要很深行业背景下的ToB任务中，全量人工标注的数据，但是只有60%~70%准确率的情况很是常见。这一方面就需要从流程机制，人员培训，标准制定等外围步骤上来解决了。有一种特殊情况，当应用场景比较复杂的时候，智能系统中需要的样本和人工理解上有较大的偏差，不理解业务的行业专家也不能保证标注数据真的可以直接应用在模型中，所以提前确定任务标注的标准非常有必要。
2. 其二是数据可用性的质量高，模型对于不同数据其实有不同的敏感度，大批量趋同模板产出的数据，其实并不能对模型产生更好的泛化性。而且其危害性可能会进一步传递，那就是下游应用在这份准确率较高的数据集上，可能会达到一个不可用的、有偏的、极好的效果。极端情况下，评估用户评论的情感任务中，对于用户有打分无回复的情况，会分别有一个默认回复。若我收集的数据99%都是默认回复，模型只要学习到两者的映射关系，就可以达到99%的准确率。尤其在现在的深度模型场景下，这个问题将更难以发现。

&emsp;&emsp;对于数据的高可用性，其实与模型、场景都是有强相关性的。如何在当前模型，当前场景下找到信息量最高的样本，是一个比较困难的问题。但是幸好我们仍然在阳光之下，业内研究的主动学习不能说是和我们的任务一样，但是也是有极大的相似度的。一个最典型的方案，就是首先通过人工来标注一部分样本，然后基于迭代的方式，通过多次模型训练，以及样本标注的过程，达到最有效果。优势是可以在更少的样本上达到更好的效果，劣势是需要标注人员频繁介入模型训练的过程，可能会减慢标注效率。

### AL常见方案
&emsp;&emsp;在[术语在线](https://www.termonline.cn/word/231141/1)中，对主动学习的定义是：学习过程中由学习器挑选未标记样本，并请求外界提供标记信息，其目标是使用尽可能少的查询来取得好的学习性能。可以发现，其中的关键词包括『学习过程中』、『学习器挑选』、『外界提供标记信息』三个。
1. 学习过程中意味着整个主动学习是一种人机交互的系统，需要在模型训练的过程中介入人工操作。
2. 学习期挑选意味着人工是作为一种辅助角色存在的，整个流程的控制交由模型和数据交互产生的结果。
3. 外界提供标记信息同样表示主动学习系统中，模型内部是没有能力来完成全部过程的，必须由另外一个系统来完成中间的过程，从而实现模型效果的提升。

----
&emsp;&emsp;常见的AL方案包括：[^1]
1. **不确定性采样的查询（Uncertainty Sampling）**：不确定性采样的查询方法就是将模型中难以区分的样本数据提取出来，提供给业务专家或者标注人员进行标注，从而达到以较快速度提升算法效果的能力。其中常见的思路包括.
    + 置信度最低（Least Confident）：所谓 Least Confident 方法就是选择那些最大概率最小的样本进行标注。
    + 边缘采样（Margin Sampling）：指的是选择那些极容易被判定成两类的样本数据，或者说这些数据被判定成两类的概率相差不大。边缘采样就是选择模型预测最大和第二大的概率差值最小的样本。其实是置信度最低方案的一种多分类扩展。
    + 熵方法（Entropy）：选择那些熵比较大的样本数据作为待定标注数据。考虑了该模型对某个输入的所有类别判定结果。
2. **基于委员会的查询（Query-By-Committee）**：除了考虑单个模型的不确定性采样方法之外，还可以考虑多个模型的场景，这就是类似集成学习的方法。通过多个模型投票的模式，来选择出那些较“难”区分的样本数据。
    + 当对每个分类器都等同对待的话，可以使用置信度最低采样或边缘采样的方式，选择某一个或某几个模型不太好区分的样本作为待标注样本。
    + 当需要考虑到每个分类器的效果时，就需要用到下面两种方案了：
        + 投票熵（Vote Entropy）：选择这些模型都无法区分的样本数据；
        + 平均 KL 散度（Average Kullback-Leibler Divergence）：选择 KL 散度较大的样本数据。
3. **基于模型变化期望的查询（Expected Model Change）**：选择那些使得梯度变化最大的样本数据。
4. **基于误差减少的查询（Expected Error Reduction）**：选择那些通过增加一个样本就使得 loss 函数减少最多的样本数据。
5. **基于方差减少的查询（Variance Reduction）**：选择那些方差减少最多的样本数据。
6. **基于密度权重的查询（Density-Weighted Methods）**：有的时候，某个数据点可能是异常点或者与大多数数据偏差较大，不太适合做样本选择或者区分，某些时候考虑那些稠密的，难以区分的数据反而价值更大。

### 应用方式
理论上对于所有具有打分信息的模型-样本系统，都可以通过主动学习的方案，来实现模型效果的优化。但是需要注意的是：若筛选条件不能正确筛选出不确定性比较大的样例，可能会导致主动学习方案不生效。

### 参考文献
[^1]: https://zhuanlan.zhihu.com/p/239756522 